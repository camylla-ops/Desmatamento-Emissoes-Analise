# Projeto final - Tema Meio ambiente 


Soulcode Academy Bootcamp Analista de Dados – Martech – AD2  

**Professores:**

- Franciane Rodrigues
- Douglas Ribeiro
- Jonathas Carneiro
  
**Integrantes:**
  
- Camila Barcellos
- Camylla Oliveira
- Ester Beatriz
- Heloisa Gasques
- Maria Eduarda Klug
- Vanessa Monteiro

## Descrição Geral do Projeto

Este é o Projeto Final do curso "Bootcamp Analista de Dados – Martech – AD2" na Soulcode Academy. Cada equipe deverá aplicar os conceitos aprendidos para tratar, organizar e modelar dados de no mínimo 2 datasets, seguindo um tema específico. Tecnologias como Google Cloud Platform (Cloud Storage), Python, Pandas, SQL, PySpark, Looker Studio, PowerBI, Big Query e MongoDB serão utilizadas obrigatoriamente.

## Apresentação

A apresentação do trabalho seguirá as seguintes diretrizes:

- Apresentação do dataset escolhido, incluindo a fonte e principais informações.
- Demonstração das funções e ferramentas utilizadas no código.
- Explicação sobre a escolha do dataset.
- Apresentação de todos os integrantes.
- Uso de terminologia técnica.
- Duração máxima de 60 minutos.

## Habilidades Avaliadas

As principais habilidades a serem avaliadas incluem:

- Comunicação em público.
- Análise SWOT.
- Uso de storytelling na apresentação dos dados.
- Identificação de métricas e KPIs relevantes.
- Sugestões de ações baseadas nas informações coletadas.
- Argumentação e capacidade de codificação em Python.
- Habilidade de interpretação e análise de dados.
- Utilização das bibliotecas Pandas e PySpark.
- Escrita de consultas SQL.
- Capacidade analítica e interpretativa.
- Trabalho em grupo e organização.

## Requisitos Obrigatórios

O projeto deverá atender aos seguintes requisitos:

- Uso de datasets com formatos diferentes (CSV / JSON / SQL / NoSQL / Excel).
- Operações com Pandas para limpeza, transformação e normalização.
- Utilização do PySpark com justificativas para as transformações.
- Pelo menos 2 tipos de gráficos para visualizar dados e inconsistências.
- Datasets devem ser traduzidos para o PT-BR.
- Armazenamento obrigatório na GCP (não usar Google Drive).
- Utilização do BigQuery e armazenamento em Datalake ou DW.
- Armazenamento dos Dataframes resultantes em um cluster MongoDB Atlas.
- Mínimo de 4 análises no Big Query com SQL.
- Criação de dashboard no Looker Studio ou PowerBI.
- Demonstração visual das etapas de ETL.
- Documentação completa do projeto.

